{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hJ10T13mWN-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LFrQoLzhWas7"
      },
      "outputs": [],
      "source": [
        "trajectory_file_path = 'rl_trajectories.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3lRUIFPWg9r"
      },
      "outputs": [],
      "source": [
        "# This is our Deep Learning model. It's a simple Multi-Layer Perceptron (MLP)\n",
        "# that takes a state and outputs a Q-value for each possible action.\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ujfe4l9TW872"
      },
      "outputs": [],
      "source": [
        "# This is our Conservative Q-Learning (CQL) agent\n",
        "\n",
        "class CQLAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, cql_alpha=5.0):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.cql_alpha = cql_alpha\n",
        "\n",
        "        # Main Q-Network\n",
        "        self.q_network = QNetwork(state_dim, action_dim)\n",
        "\n",
        "        # Target Q-Network for stability\n",
        "        self.target_q_network = QNetwork(state_dim, action_dim)\n",
        "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "\n",
        "    def train(self, batch):\n",
        "        states = torch.FloatTensor(np.array(batch['state'].tolist()))\n",
        "        actions = torch.LongTensor(batch['action'].tolist()).unsqueeze(1)\n",
        "        rewards = torch.FloatTensor(batch['reward'].tolist()).unsqueeze(1)\n",
        "        next_states = torch.FloatTensor(np.array(batch['next_state'].tolist()))\n",
        "        terminals = torch.FloatTensor(batch['terminal'].tolist()).unsqueeze(1)\n",
        "\n",
        "        # Standard Q-Learning Loss (Bellman Error)\n",
        "        # Get Q-values for the actions that were actually taken in the dataset\n",
        "        q_values = self.q_network(states).gather(1, actions)\n",
        "\n",
        "        # Get the value of the next state from the target network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_q_network(next_states).max(1)[0].unsqueeze(1)\n",
        "            target_q_values = rewards + (1 - terminals) * self.gamma * next_q_values\n",
        "\n",
        "        q_loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "        # CQL Conservative Loss\n",
        "        # It penalizes Q-values for actions that were NOT in the dataset\n",
        "        # forcing the model to be \"conservative.\"\n",
        "\n",
        "        all_q_values = self.q_network(states)\n",
        "\n",
        "        logsumexp_q = torch.logsumexp(all_q_values, dim=1, keepdim=True)\n",
        "\n",
        "        dataset_q_values = q_values\n",
        "\n",
        "        # The CQL loss encourages the Q-values of actions in the dataset to be high,\n",
        "        # while pushing down the Q-values of other actions.\n",
        "        cql_loss = (logsumexp_q - dataset_q_values).mean()\n",
        "\n",
        "        # The final loss is a combination of the standard Q-loss and the CQL penalty\n",
        "        total_loss = q_loss + self.cql_alpha * cql_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()\n",
        "\n",
        "    def update_target_network(self, tau=0.005):\n",
        "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsVLxWYPXvnb",
        "outputId": "46e1136c-820f-447a-e9e9-859334437f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Model Training\n",
            "Detected State Dimension: 10\n",
            "Detected Action Dimension: 5\n",
            "Epoch 100/1000, Loss: 36.0201\n",
            "Epoch 200/1000, Loss: 28.9910\n",
            "Epoch 300/1000, Loss: 28.0143\n",
            "Epoch 400/1000, Loss: 30.0036\n",
            "Epoch 500/1000, Loss: 28.8302\n",
            "Epoch 600/1000, Loss: 26.0545\n",
            "Epoch 700/1000, Loss: 26.4429\n",
            "Epoch 800/1000, Loss: 22.3608\n",
            "Epoch 900/1000, Loss: 28.2073\n",
            "Epoch 1000/1000, Loss: 28.5363\n",
            "\n",
            "--- Training Complete ---\n"
          ]
        }
      ],
      "source": [
        "# The Training Loop\n",
        "print(\"Starting Model Training\")\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1000\n",
        "\n",
        "dataset = pd.read_pickle(trajectory_file_path)\n",
        "\n",
        "state_dim = len(dataset['state'].iloc[0])\n",
        "action_dim = dataset['action'].max() + 1\n",
        "\n",
        "print(f\"Detected State Dimension: {state_dim}\")\n",
        "print(f\"Detected Action Dimension: {action_dim}\")\n",
        "\n",
        "agent = CQLAgent(state_dim=state_dim, action_dim=action_dim)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    batch = dataset.sample(n=BATCH_SIZE)\n",
        "\n",
        "    loss = agent.train(batch)\n",
        "\n",
        "    agent.update_target_network()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWIKfTgBUk5D",
        "outputId": "97d561e8-8ca1-4a78-9e9b-920bcc6ca770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model saved successfully to 'cql_fincoach_model.pth'\n"
          ]
        }
      ],
      "source": [
        "model_save_path = \"cql_fincoach_model.pth\"\n",
        "torch.save(agent.q_network.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"\\nModel saved successfully to '{model_save_path}'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
